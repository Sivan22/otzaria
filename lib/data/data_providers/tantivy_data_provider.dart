import 'dart:io';
import 'dart:math' as math;
import 'package:flutter/foundation.dart';
import 'package:search_engine/search_engine.dart';
import 'package:flutter_settings_screens/flutter_settings_screens.dart';
import 'package:hive/hive.dart';
import 'package:otzaria/search/search_repository.dart';

/// A singleton class that manages search functionality using Tantivy search engine.
///
/// This provider handles the search operations for both text-based and PDF books,
/// maintaining an index for full-text search capabilities.
class TantivyDataProvider {
  /// Instance of the search engine pointing to the index directory
  late Future<SearchEngine> engine;
  late ReferenceSearchEngine refEngine;

  static final TantivyDataProvider _singleton = TantivyDataProvider();
  static TantivyDataProvider instance = _singleton;

  // Global cache for facet counts
  static final Map<String, int> _globalFacetCache = {};
  static String _lastCachedQuery = '';

  // Track ongoing counts to prevent duplicates
  static final Set<String> _ongoingCounts = {};

  /// Clear global cache when starting new search
  static void clearGlobalCache() {
    print(
        'ğŸ§¹ Clearing global facet cache (${_globalFacetCache.length} entries)');
    _globalFacetCache.clear();
    _ongoingCounts.clear();
    _lastCachedQuery = '';
  }

  /// Indicates whether the indexing process is currently running
  ValueNotifier<bool> isIndexing = ValueNotifier(false);

  /// Maintains a list of processed books to avoid reindexing
  late List<String> booksDone;

  TantivyDataProvider() {
    reopenIndex();
  }

  void reopenIndex() {
    String indexPath = (Settings.getValue('key-library-path') ?? 'C:/××•×¦×¨×™×') +
        Platform.pathSeparator +
        'index';
    String refIndexPath =
        (Settings.getValue('key-library-path') ?? 'C:/××•×¦×¨×™×') +
            Platform.pathSeparator +
            'ref_index';

    engine = Future.value(SearchEngine(path: indexPath));

    try {
      refEngine = ReferenceSearchEngine(path: refIndexPath);
    } catch (e) {
      if (e.toString() ==
          "PanicException(Failed to create index: SchemaError(\"An index exists but the schema does not match.\"))") {
        resetIndex(indexPath);
        reopenIndex();
      } else {
        rethrow;
      }
    }
    //test the engine
    engine.then((value) {
      try {
        // Test the search engine
        value
            .search(
                regexTerms: ['a'],
                limit: 10,
                slop: 0,
                maxExpansions: 10,
                facets: ["/"],
                order: ResultsOrder.catalogue)
            .then((results) {
          // Engine test successful
        }).catchError((e) {
          // Log engine test error
        });
      } catch (e) {
        // Log sync engine test error
        if (e.toString() ==
            "PanicException(Failed to create index: SchemaError(\"An index exists but the schema does not match.\"))") {
          resetIndex(indexPath);
          reopenIndex();
        } else {
          rethrow;
        }
      }
    });
    try {
      booksDone = Hive.box(
              name: 'books_indexed',
              directory:
                  (Settings.getValue('key-library-path') ?? 'C:/××•×¦×¨×™×') +
                      Platform.pathSeparator +
                      'index')
          .get('key-books-done', defaultValue: [])
          .map<String>((e) => e.toString())
          .toList() as List<String>;
    } catch (e) {
      booksDone = [];
    }
  }

  /// Persists the list of indexed books to disk using Hive storage.
  saveBooksDoneToDisk() {
    Hive.box(
            name: 'books_indexed',
            directory: (Settings.getValue('key-library-path') ?? 'C:/××•×¦×¨×™×') +
                Platform.pathSeparator +
                'index')
        .put('key-books-done', booksDone);
  }

  Future<int> countTexts(String query, List<String> books, List<String> facets,
      {bool fuzzy = false,
      int distance = 2,
      Map<String, String>? customSpacing,
      Map<int, List<String>>? alternativeWords,
      Map<String, Map<String, bool>>? searchOptions}) async {
    // Global cache check
    final cacheKey =
        '$query|${facets.join(',')}|$fuzzy|$distance|${customSpacing.toString()}|${alternativeWords.toString()}|${searchOptions.toString()}';

    if (_lastCachedQuery == query && _globalFacetCache.containsKey(cacheKey)) {
      print('ğŸ¯ GLOBAL CACHE HIT for $facets: ${_globalFacetCache[cacheKey]}');
      return _globalFacetCache[cacheKey]!;
    }

    // Check if this count is already in progress
    if (_ongoingCounts.contains(cacheKey)) {
      print('â³ Count already in progress for $facets, waiting...');
      // Wait for the ongoing count to complete
      while (_ongoingCounts.contains(cacheKey)) {
        await Future.delayed(const Duration(milliseconds: 50));
        if (_globalFacetCache.containsKey(cacheKey)) {
          print(
              'ğŸ¯ DELAYED CACHE HIT for $facets: ${_globalFacetCache[cacheKey]}');
          return _globalFacetCache[cacheKey]!;
        }
      }
    }

    // Mark this count as in progress
    _ongoingCounts.add(cacheKey);
    final index = await engine;

    // ×‘×“×™×§×” ×× ×™×© ××¨×•×•×—×™× ××•×ª×××™× ××™×©×™×ª, ××™×œ×™× ×—×™×œ×•×¤×™×•×ª ××• ××¤×©×¨×•×™×•×ª ×—×™×¤×•×©
    final hasCustomSpacing = customSpacing != null && customSpacing.isNotEmpty;
    final hasAlternativeWords =
        alternativeWords != null && alternativeWords.isNotEmpty;
    final hasSearchOptions = searchOptions != null && searchOptions.isNotEmpty;

    // ×”××¨×ª ×”×—×™×¤×•×© ×œ×¤×•×¨××˜ ×”×× ×•×¢ ×”×—×“×© - ×‘×“×™×•×§ ×›××• ×‘-SearchRepository!
    final words = query.trim().split(RegExp(r'\s+'));
    final List<String> regexTerms;
    final int effectiveSlop;

    if (hasAlternativeWords || hasSearchOptions) {
      // ×™×© ××™×œ×™× ×—×™×œ×•×¤×™×•×ª ××• ××¤×©×¨×•×™×•×ª ×—×™×¤×•×© - × ×‘× ×” queries ××ª×§×“××™×
      regexTerms =
          _buildAdvancedQueryForCount(words, alternativeWords, searchOptions);
      effectiveSlop = hasCustomSpacing
          ? _getMaxCustomSpacingForCount(customSpacing, words.length)
          : (fuzzy ? distance : 0);
    } else if (fuzzy) {
      // ×—×™×¤×•×© ××§×•×¨×‘ - × ×©×ª××© ×‘××™×œ×™× ×‘×•×“×“×•×ª
      regexTerms = words;
      effectiveSlop = distance;
    } else if (words.length == 1) {
      // ××™×œ×” ××—×ª - ×—×™×¤×•×© ×¤×©×•×˜
      regexTerms = [query];
      effectiveSlop = 0;
    } else if (hasCustomSpacing) {
      // ××¨×•×•×—×™× ××•×ª×××™× ××™×©×™×ª
      regexTerms = words;
      effectiveSlop = _getMaxCustomSpacingForCount(customSpacing, words.length);
    } else {
      // ×—×™×¤×•×© ××“×•×™×™×§ ×©×œ ×›××” ××™×œ×™×
      regexTerms = words;
      effectiveSlop = distance;
    }

    // ×—×™×©×•×‘ maxExpansions ×‘×”×ª×‘×¡×¡ ×¢×œ ×¡×•×’ ×”×—×™×¤×•×©
    final int maxExpansions = _calculateMaxExpansionsForCount(
        fuzzy, regexTerms.length,
        searchOptions: searchOptions, words: words);

    try {
      final count = await index.count(
          regexTerms: regexTerms,
          facets: facets,
          slop: effectiveSlop,
          maxExpansions: maxExpansions);

      // Save to global cache
      _lastCachedQuery = query;
      _globalFacetCache[cacheKey] = count;
      _ongoingCounts.remove(cacheKey); // Mark as completed
      print('ğŸ’¾ GLOBAL CACHE SAVE for $facets: $count');

      return count;
    } catch (e) {
      // Remove from ongoing counts even on error
      _ongoingCounts.remove(cacheKey);
      // Log error in production
      rethrow;
    }
  }

  Future<void> resetIndex(String indexPath) async {
    Directory indexDirectory = Directory(indexPath);
    Hive.box(name: 'books_indexed', directory: indexPath).close();
    indexDirectory.deleteSync(recursive: true);
    indexDirectory.createSync(recursive: true);
  }

  /// Performs an asynchronous stream-based search operation across indexed texts.
  ///
  /// [query] The search query string
  /// [books] List of book identifiers to search within
  /// [limit] Maximum number of results to return
  /// [fuzzy] Whether to perform fuzzy matching
  ///
  /// Returns a Stream of search results that can be listened to for real-time updates
  Stream<List<SearchResult>> searchTextsStream(
      String query, List<String> facets, int limit, bool fuzzy) async* {
    // ×”×¤×•× ×§×¦×™×” ×”×–×• ×œ× × ×ª××›×ª ×‘×× ×•×¢ ×”×—×“×© - × ×—×–×™×¨ ×ª×•×¦××” ×—×“-×¤×¢××™×ª
    final searchRepository = SearchRepository();
    final results =
        await searchRepository.searchTexts(query, facets, limit, fuzzy: fuzzy);
    yield results;
  }

  Future<List<ReferenceSearchResult>> searchRefs(
      String reference, int limit, bool fuzzy) async {
    return refEngine.search(
        query: reference,
        limit: limit,
        fuzzy: fuzzy,
        order: ResultsOrder.relevance);
  }

  /// ××—×©×‘ ××ª ×”××¨×•×•×— ×”××§×¡×™××œ×™ ××”××¨×•×•×—×™× ×”××•×ª×××™× ××™×©×™×ª
  int _getMaxCustomSpacingForCount(
      Map<String, String> customSpacing, int wordCount) {
    int maxSpacing = 0;

    for (int i = 0; i < wordCount - 1; i++) {
      final spacingKey = '$i-${i + 1}';
      final customSpacingValue = customSpacing[spacingKey];

      if (customSpacingValue != null && customSpacingValue.isNotEmpty) {
        final spacingNum = int.tryParse(customSpacingValue) ?? 0;
        maxSpacing = maxSpacing > spacingNum ? maxSpacing : spacingNum;
      }
    }

    return maxSpacing;
  }

  /// ×‘×•× ×” query ××ª×§×“× ×¢× ××™×œ×™× ×—×™×œ×•×¤×™×•×ª ×•××¤×©×¨×•×™×•×ª ×—×™×¤×•×©
  List<String> _buildAdvancedQueryForCount(
      List<String> words,
      Map<int, List<String>>? alternativeWords,
      Map<String, Map<String, bool>>? searchOptions) {
    List<String> regexTerms = [];

    for (int i = 0; i < words.length; i++) {
      final word = words[i];
      final wordKey = '${word}_$i';

      // ×§×‘×œ×ª ××¤×©×¨×•×™×•×ª ×”×—×™×¤×•×© ×œ××™×œ×” ×”×–×•
      final wordOptions = searchOptions?[wordKey] ?? {};
      final hasPrefix = wordOptions['×§×™×“×•××•×ª'] == true;
      final hasSuffix = wordOptions['×¡×™×•××•×ª'] == true;
      final hasGrammaticalPrefixes = wordOptions['×§×™×“×•××•×ª ×“×§×“×•×§×™×•×ª'] == true;
      final hasGrammaticalSuffixes = wordOptions['×¡×™×•××•×ª ×“×§×“×•×§×™×•×ª'] == true;
      final hasFullPartialSpelling = wordOptions['×›×ª×™×‘ ××œ×/×—×¡×¨'] == true;
      final hasPartialWord = wordOptions['×—×œ×§ ×××™×œ×”'] == true;

      // ×§×‘×œ×ª ××™×œ×™× ×—×™×œ×•×¤×™×•×ª
      final alternatives = alternativeWords?[i];

      // ×‘× ×™×™×ª ×¨×©×™××ª ×›×œ ×”××¤×©×¨×•×™×•×ª (××™×œ×” ××§×•×¨×™×ª + ×—×œ×•×¤×•×ª)
      final allOptions = [word];
      if (alternatives != null && alternatives.isNotEmpty) {
        allOptions.addAll(alternatives);
      }

      // ×¡×™× ×•×Ÿ ××¤×©×¨×•×™×•×ª ×¨×™×§×•×ª
      final validOptions =
          allOptions.where((w) => w.trim().isNotEmpty).toList();

      if (validOptions.isNotEmpty) {
        // ×‘× ×™×™×ª ×¨×©×™××ª ×›×œ ×”××¤×©×¨×•×™×•×ª ×œ×›×œ ××™×œ×”
        final allVariations = <String>{};

        for (final option in validOptions) {
          List<String> baseVariations = [option];

          // ×× ×™×© ×›×ª×™×‘ ××œ×/×—×¡×¨, × ×•×¦×¨ ××ª ×›×œ ×”×•×•×¨×™××¦×™×•×ª ×©×œ ×›×ª×™×‘
          if (hasFullPartialSpelling) {
            try {
              // ×”×’×‘×œ×” ×œ××™×œ×™× ×§×¦×¨×•×ª - ×›×ª×™×‘ ××œ×/×—×¡×¨ ×™×›×•×œ ×œ×™×¦×•×¨ ×”×¨×‘×” ×•×¨×™××¦×™×•×ª
              if (option.length <= 3) {
                // ×œ××™×œ×™× ×§×¦×¨×•×ª, × ×’×‘×™×œ ××ª ××¡×¤×¨ ×”×•×¨×™××¦×™×•×ª
                final allSpellingVariations =
                    _generateFullPartialSpellingVariations(option);
                // × ×§×— ×¨×§ ××ª ×”-5 ×”×¨××©×•× ×•×ª ×›×“×™ ×œ×× ×•×¢ ×™×•×ª×¨ ××“×™ expansions
                baseVariations = allSpellingVariations.take(5).toList();
              } else {
                baseVariations = _generateFullPartialSpellingVariations(option);
              }
            } catch (e) {
              // ×× ×™×© ×‘×¢×™×”, × ×•×¡×™×£ ×œ×¤×—×•×ª ××ª ×”××™×œ×” ×”××§×•×¨×™×ª
              baseVariations = [option];
            }
          }

          // ×¢×‘×•×¨ ×›×œ ×•×¨×™××¦×™×” ×©×œ ×›×ª×™×‘, ××•×¡×™×¤×™× ××ª ×”××¤×©×¨×•×™×•×ª ×”×“×§×“×•×§×™×•×ª
          for (final baseVariation in baseVariations) {
            if (hasGrammaticalPrefixes && hasGrammaticalSuffixes) {
              // ×©×ª×™ ×”××¤×©×¨×•×™×•×ª ×™×—×“ - ×”×’×‘×œ×” ×œ××™×œ×™× ×§×¦×¨×•×ª
              if (baseVariation.length <= 2) {
                // ×œ××™×œ×™× ×§×¦×¨×•×ª, × ×©×ª××© ×‘×¨×’×§×¡ ×§×•××¤×§×˜×™ ×‘××§×•× ×¨×©×™××ª ×•×¨×™××¦×™×•×ª
                allVariations
                    .add(_createFullMorphologicalRegexPattern(baseVariation));
              } else {
                allVariations.addAll(
                    _generateFullMorphologicalVariations(baseVariation));
              }
            } else if (hasGrammaticalPrefixes) {
              // ×¨×§ ×§×™×“×•××•×ª ×“×§×“×•×§×™×•×ª - ×”×’×‘×œ×” ×œ××™×œ×™× ×§×¦×¨×•×ª
              if (baseVariation.length <= 2) {
                // ×œ××™×œ×™× ×§×¦×¨×•×ª, × ×©×ª××© ×‘×¨×’×§×¡ ×§×•××¤×§×˜×™
                allVariations.add(_createPrefixRegexPattern(baseVariation));
              } else {
                allVariations.addAll(_generatePrefixVariations(baseVariation));
              }
            } else if (hasGrammaticalSuffixes) {
              // ×¨×§ ×¡×™×•××•×ª ×“×§×“×•×§×™×•×ª - ×”×’×‘×œ×” ×œ××™×œ×™× ×§×¦×¨×•×ª
              if (baseVariation.length <= 2) {
                // ×œ××™×œ×™× ×§×¦×¨×•×ª, × ×©×ª××© ×‘×¨×’×§×¡ ×§×•××¤×§×˜×™
                allVariations.add(_createSuffixRegexPattern(baseVariation));
              } else {
                allVariations.addAll(_generateSuffixVariations(baseVariation));
              }
            } else if (hasPrefix) {
              // ×§×™×“×•××•×ª ×¨×’×™×œ×•×ª - ×”×’×‘×œ×” ×—×›××” ×œ×¤×™ ××•×¨×š ×”××™×œ×”
              if (baseVariation.length <= 1) {
                // ××™×œ×” ×©×œ ×ª×• ××—×“ - ×”×’×‘×œ×” ×§×™×¦×•× ×™×ª (××§×¡×™××•× 5 ×ª×•×•×™× ×§×™×“×•××ª)
                allVariations.add('.{1,5}${RegExp.escape(baseVariation)}');
              } else if (baseVariation.length <= 2) {
                // ××™×œ×” ×©×œ 2 ×ª×•×•×™× - ×”×’×‘×œ×” ×‘×™× ×•× ×™×ª (××§×¡×™××•× 4 ×ª×•×•×™× ×§×™×“×•××ª)
                allVariations.add('.{1,4}${RegExp.escape(baseVariation)}');
              } else if (baseVariation.length <= 3) {
                // ××™×œ×” ×©×œ 3 ×ª×•×•×™× - ×”×’×‘×œ×” ×§×œ×” (××§×¡×™××•× 3 ×ª×•×•×™× ×§×™×“×•××ª)
                allVariations.add('.{1,3}${RegExp.escape(baseVariation)}');
              } else {
                // ××™×œ×” ××¨×•×›×” - ×œ×œ× ×”×’×‘×œ×”
                allVariations.add('.*${RegExp.escape(baseVariation)}');
              }
            } else if (hasSuffix) {
              // ×¡×™×•××•×ª ×¨×’×™×œ×•×ª - ×”×’×‘×œ×” ×—×›××” ×œ×¤×™ ××•×¨×š ×”××™×œ×”
              if (baseVariation.length <= 1) {
                // ××™×œ×” ×©×œ ×ª×• ××—×“ - ×”×’×‘×œ×” ×§×™×¦×•× ×™×ª (××§×¡×™××•× 7 ×ª×•×•×™× ×¡×™×•××ª)
                allVariations.add('${RegExp.escape(baseVariation)}.{1,7}');
              } else if (baseVariation.length <= 2) {
                // ××™×œ×” ×©×œ 2 ×ª×•×•×™× - ×”×’×‘×œ×” ×‘×™× ×•× ×™×ª (××§×¡×™××•× 6 ×ª×•×•×™× ×¡×™×•××ª)
                allVariations.add('${RegExp.escape(baseVariation)}.{1,6}');
              } else if (baseVariation.length <= 3) {
                // ××™×œ×” ×©×œ 3 ×ª×•×•×™× - ×”×’×‘×œ×” ×§×œ×” (××§×¡×™××•× 5 ×ª×•×•×™× ×¡×™×•××ª)
                allVariations.add('${RegExp.escape(baseVariation)}.{1,5}');
              } else {
                // ××™×œ×” ××¨×•×›×” - ×œ×œ× ×”×’×‘×œ×”
                allVariations.add('${RegExp.escape(baseVariation)}.*');
              }
            } else if (hasPartialWord) {
              // ×—×œ×§ ×××™×œ×” - ×”×’×‘×œ×” ×—×›××” ×œ×¤×™ ××•×¨×š ×”××™×œ×”
              if (baseVariation.length <= 3) {
                // ××™×œ×” ×§×¦×¨×” (1-3 ×ª×•×•×™×) - 3 ×ª×•×•×™× ×œ×¤× ×™ ×•3 ××—×¨×™
                allVariations.add('.{0,3}${RegExp.escape(baseVariation)}.{0,3}');
              } else {
                // ××™×œ×” ××¨×•×›×” (4+ ×ª×•×•×™×) - 2 ×ª×•×•×™× ×œ×¤× ×™ ×•2 ××—×¨×™
                allVariations.add('.{0,2}${RegExp.escape(baseVariation)}.{0,2}');
              }
            } else {
              // ×œ×œ× ××¤×©×¨×•×™×•×ª ××™×•×—×“×•×ª - ××™×œ×” ××“×•×™×§×ª
              allVariations.add(RegExp.escape(baseVariation));
            }
          }
        }

        // ×”×’×‘×œ×” ×¢×œ ××¡×¤×¨ ×”×•×¨×™××¦×™×•×ª ×”×›×•×œ×œ ×œ××™×œ×” ××—×ª
        final limitedVariations = allVariations.length > 20
            ? allVariations.take(20).toList()
            : allVariations.toList();

        // ×‘××§×•× ×¨×’×§×¡ ××•×¨×›×‘, × ×•×¡×™×£ ×›×œ ×•×¨×™××¦×™×” ×‘× ×¤×¨×“
        final finalPattern = limitedVariations.length == 1
            ? limitedVariations.first
            : '(${limitedVariations.join('|')})';

        regexTerms.add(finalPattern);
      } else {
        // fallback ×œ××™×œ×” ×”××§×•×¨×™×ª
        regexTerms.add(word);
      }
    }

    return regexTerms;
  }

  /// ××—×©×‘ ××ª maxExpansions ×‘×”×ª×‘×¡×¡ ×¢×œ ×¡×•×’ ×”×—×™×¤×•×©
  int _calculateMaxExpansionsForCount(bool fuzzy, int termCount,
      {Map<String, Map<String, bool>>? searchOptions, List<String>? words}) {
    // ×‘×“×™×§×” ×× ×™×© ×—×™×¤×•×© ×¢× ×¡×™×•××•×ª ××• ×§×™×“×•××•×ª ×•××™×–×” ××™×œ×™×
    bool hasSuffixOrPrefix = false;
    int shortestWordLength = 10; // ×¢×¨×š ×”×ª×—×œ×ª×™ ×’×‘×•×”

    if (searchOptions != null && words != null) {
      for (int i = 0; i < words.length; i++) {
        final word = words[i];
        final wordKey = '${word}_$i';
        final wordOptions = searchOptions[wordKey] ?? {};

        if (wordOptions['×¡×™×•××•×ª'] == true ||
            wordOptions['×§×™×“×•××•×ª'] == true ||
            wordOptions['×§×™×“×•××•×ª ×“×§×“×•×§×™×•×ª'] == true ||
            wordOptions['×¡×™×•××•×ª ×“×§×“×•×§×™×•×ª'] == true ||
            wordOptions['×—×œ×§ ×××™×œ×”'] == true) {
          hasSuffixOrPrefix = true;
          shortestWordLength = math.min(shortestWordLength, word.length);
        }
      }
    }

    if (fuzzy) {
      return 50; // ×—×™×¤×•×© ××§×•×¨×‘
    } else if (hasSuffixOrPrefix) {
      // ×”×ª×××ª ×”××’×‘×œ×” ×œ×¤×™ ××•×¨×š ×”××™×œ×” ×”×§×¦×¨×” ×‘×™×•×ª×¨ ×¢× ××¤×©×¨×•×™×•×ª ××ª×§×“××•×ª
      if (shortestWordLength <= 1) {
        return 2000; // ××™×œ×” ×©×œ ×ª×• ××—×“ - ×”×’×‘×œ×” ×§×™×¦×•× ×™×ª
      } else if (shortestWordLength <= 2) {
        return 3000; // ××™×œ×” ×©×œ 2 ×ª×•×•×™× - ×”×’×‘×œ×” ×‘×™× ×•× ×™×ª
      } else if (shortestWordLength <= 3) {
        return 4000; // ××™×œ×” ×©×œ 3 ×ª×•×•×™× - ×”×’×‘×œ×” ×§×œ×”
      } else {
        return 5000; // ××™×œ×” ××¨×•×›×” - ×”×’×‘×œ×” ××œ××”
      }
    } else if (termCount > 1) {
      return 100; // ×—×™×¤×•×© ×©×œ ×›××” ××™×œ×™× - ×¦×¨×™×š expansions ×’×‘×•×” ×™×•×ª×¨
    } else {
      return 10; // ××™×œ×” ××—×ª - expansions × ××•×š
    }
  }

  // ×¤×•× ×§×¦×™×•×ª ×¢×–×¨ ×œ×™×¦×™×¨×ª ×•×¨×™××¦×™×•×ª ×›×ª×™×‘ ××œ×/×—×¡×¨ ×•×“×§×“×•×§×™×•×ª
  List<String> _generateFullPartialSpellingVariations(String word) {
    if (word.isEmpty) return [word];

    final variations = <String>{word}; // ×”××™×œ×” ×”××§×•×¨×™×ª

    // ××•×¦× ××ª ×›×œ ×”××™×§×•××™× ×©×œ ×™, ×•, ×•×’×¨×©×™×™×
    final chars = word.split('');
    final optionalIndices = <int>[];

    // ××•×¦× ××™× ×“×§×¡×™× ×©×œ ×ª×•×•×™× ×©×™×›×•×œ×™× ×œ×”×™×•×ª ××•×¤×¦×™×•× ×œ×™×™×
    for (int i = 0; i < chars.length; i++) {
      if (chars[i] == '×™' ||
          chars[i] == '×•' ||
          chars[i] == "'" ||
          chars[i] == '"') {
        optionalIndices.add(i);
      }
    }

    // ×™×•×¦×¨ ××ª ×›×œ ×”×¦×™×¨×•×¤×™× ×”××¤×©×¨×™×™× (2^n ××¤×©×¨×•×™×•×ª)
    final numCombinations = 1 << optionalIndices.length; // 2^n

    for (int combination = 0; combination < numCombinations; combination++) {
      final variant = <String>[];

      for (int i = 0; i < chars.length; i++) {
        final optionalIndex = optionalIndices.indexOf(i);

        if (optionalIndex != -1) {
          // ×–×” ×ª×• ××•×¤×¦×™×•× ×œ×™ - ×‘×•×“×§ ×× ×œ×›×œ×•×œ ××•×ª×• ×‘×¦×™×¨×•×£ ×”×–×”
          final shouldInclude = (combination & (1 << optionalIndex)) != 0;
          if (shouldInclude) {
            variant.add(chars[i]);
          }
        } else {
          // ×ª×• ×¨×’×™×œ - ×ª××™×“ ×›×•×œ×œ
          variant.add(chars[i]);
        }
      }

      variations.add(variant.join(''));
    }

    return variations.toList();
  }

  List<String> _generateFullMorphologicalVariations(String word) {
    // ×¤×•× ×§×¦×™×” ×¤×©×•×˜×” ×©××—×–×™×¨×” ××ª ×”××™×œ×” ×¢× ×§×™×“×•××•×ª ×•×¡×™×•××•×ª ×‘×¡×™×¡×™×•×ª
    final variations = <String>{word};

    // ×§×™×“×•××•×ª ×‘×¡×™×¡×™×•×ª
    final prefixes = ['×‘', '×”', '×•', '×›', '×œ', '×', '×©'];
    // ×¡×™×•××•×ª ×‘×¡×™×¡×™×•×ª
    final suffixes = ['×”', '×™×', '×•×ª', '×™', '×š', '× ×•', '×›×', '×”×'];

    // ×”×•×¡×¤×ª ×§×™×“×•××•×ª
    for (final prefix in prefixes) {
      variations.add('$prefix$word');
    }

    // ×”×•×¡×¤×ª ×¡×™×•××•×ª
    for (final suffix in suffixes) {
      variations.add('$word$suffix');
    }

    // ×”×•×¡×¤×ª ×§×™×“×•××•×ª ×•×¡×™×•××•×ª ×™×—×“
    for (final prefix in prefixes) {
      for (final suffix in suffixes) {
        variations.add('$prefix$word$suffix');
      }
    }

    return variations.toList();
  }

  List<String> _generatePrefixVariations(String word) {
    final variations = <String>{word};
    final prefixes = ['×‘', '×”', '×•', '×›', '×œ', '×', '×©'];

    for (final prefix in prefixes) {
      variations.add('$prefix$word');
    }

    return variations.toList();
  }

  List<String> _generateSuffixVariations(String word) {
    final variations = <String>{word};
    final suffixes = ['×”', '×™×', '×•×ª', '×™', '×š', '× ×•', '×›×', '×”×'];

    for (final suffix in suffixes) {
      variations.add('$word$suffix');
    }

    return variations.toList();
  }

  /// ×¡×¤×™×¨×” ××§×‘×¦×ª ×©×œ ×ª×•×¦××•×ª ×¢×‘×•×¨ ××¡×¤×¨ facets ×‘×‘×ª ××—×ª - ×œ×©×™×¤×•×¨ ×‘×™×¦×•×¢×™×
  Future<Map<String, int>> countTextsForMultipleFacets(
      String query, List<String> books, List<String> facets,
      {bool fuzzy = false,
      int distance = 2,
      Map<String, String>? customSpacing,
      Map<int, List<String>>? alternativeWords,
      Map<String, Map<String, bool>>? searchOptions}) async {
    print(
        'ğŸ” TantivyDataProvider: Starting batch count for ${facets.length} facets');
    final stopwatch = Stopwatch()..start();

    final index = await engine;
    final results = <String, int>{};

    // ×‘×“×™×§×” ×× ×™×© ××¨×•×•×—×™× ××•×ª×××™× ××™×©×™×ª, ××™×œ×™× ×—×™×œ×•×¤×™×•×ª ××• ××¤×©×¨×•×™×•×ª ×—×™×¤×•×©
    final hasCustomSpacing = customSpacing != null && customSpacing.isNotEmpty;
    final hasAlternativeWords =
        alternativeWords != null && alternativeWords.isNotEmpty;
    final hasSearchOptions = searchOptions != null && searchOptions.isNotEmpty;

    // ×”××¨×ª ×”×—×™×¤×•×© ×œ×¤×•×¨××˜ ×”×× ×•×¢ ×”×—×“×© - ×‘×“×™×•×§ ×›××• ×‘-countTexts
    final words = query.trim().split(RegExp(r'\s+'));
    final List<String> regexTerms;
    final int effectiveSlop;

    if (hasAlternativeWords || hasSearchOptions) {
      regexTerms =
          _buildAdvancedQueryForCount(words, alternativeWords, searchOptions);
      effectiveSlop = hasCustomSpacing
          ? _getMaxCustomSpacingForCount(customSpacing, words.length)
          : (fuzzy ? distance : 0);
    } else if (fuzzy) {
      regexTerms = words;
      effectiveSlop = distance;
    } else if (words.length == 1) {
      regexTerms = [query];
      effectiveSlop = 0;
    } else if (hasCustomSpacing) {
      regexTerms = words;
      effectiveSlop = _getMaxCustomSpacingForCount(customSpacing, words.length);
    } else {
      regexTerms = words;
      effectiveSlop = distance;
    }

    final int maxExpansions = _calculateMaxExpansionsForCount(
        fuzzy, regexTerms.length,
        searchOptions: searchOptions, words: words);

    // ×‘×™×¦×•×¢ ×¡×¤×™×¨×” ×¢×‘×•×¨ ×›×œ facet - ×‘×–×” ××—×¨ ×–×” (×œ× ×‘××§×‘×™×œ ×›×™ ×–×” ×œ× ×¢×•×‘×“)
    int processedCount = 0;
    int zeroResultsCount = 0;

    for (final facet in facets) {
      try {
        print(
            'ğŸ” Counting facet: $facet (${processedCount + 1}/${facets.length})');
        final facetStopwatch = Stopwatch()..start();
        final count = await index.count(
            regexTerms: regexTerms,
            facets: [facet],
            slop: effectiveSlop,
            maxExpansions: maxExpansions);
        facetStopwatch.stop();
        print(
            'âœ… Facet $facet: $count (${facetStopwatch.elapsedMilliseconds}ms)');
        results[facet] = count;

        processedCount++;
        if (count == 0) {
          zeroResultsCount++;
        }

        // ×× ×™×© ×™×•×ª×¨ ××“×™ facets ×¢× 0 ×ª×•×¦××•×ª, × ×¤×¡×™×§ ××•×§×“×
        if (processedCount >= 10 && zeroResultsCount > processedCount * 0.8) {
          print('âš ï¸ Too many zero results, stopping early');
          // × ××œ× ××ª ×”×©××¨ ×¢× 0
          for (int i = processedCount; i < facets.length; i++) {
            results[facets[i]] = 0;
          }
          break;
        }
      } catch (e) {
        print('âŒ Error counting facet $facet: $e');
        results[facet] = 0;
        processedCount++;
        zeroResultsCount++;
      }
    }

    stopwatch.stop();
    print(
        'âœ… TantivyDataProvider: Batch count completed in ${stopwatch.elapsedMilliseconds}ms');
    print(
        'ğŸ“Š Results: ${results.entries.where((e) => e.value > 0).map((e) => '${e.key}: ${e.value}').join(', ')}');

    return results;
  }

  /// Clears the index and resets the list of indexed books.
  Future<void> clear() async {
    isIndexing.value = false;
    final index = await engine;
    await index.clear();
    final refIndex = refEngine;
    await refIndex.clear();
    booksDone.clear();
    saveBooksDoneToDisk();
  }

  // ×¤×•× ×§×¦×™×•×ª ×¢×–×¨ ×œ×™×¦×™×¨×ª regex ×§×•××¤×§×˜×™ ×œ×—×™×¤×•×©×™× ×“×§×“×•×§×™×™×

  /// ×™×•×¦×¨ ×“×¤×•×¡ ×¨×’×§×¡ ×§×•××¤×§×˜×™ ×œ×—×™×¤×•×© ××™×œ×” ×¢× ×§×™×“×•××•×ª ×“×§×“×•×§×™×•×ª
  String _createPrefixRegexPattern(String word) {
    if (word.isEmpty) return word;
    // ×©×™××•×© ×‘×ª×‘× ×™×ª ×§×‘×•×¢×” ×•×™×¢×™×œ×” - ××•×’×‘×œ×ª ×œ×§×™×“×•××•×ª × ×¤×•×¦×•×ª
    return r'(×•|×|×›|×‘|×©|×œ|×”|×“)?(×›|×‘|×©|×œ|×”|×“)?(×”)?' + RegExp.escape(word);
  }

  /// ×™×•×¦×¨ ×“×¤×•×¡ ×¨×’×§×¡ ×§×•××¤×§×˜×™ ×œ×—×™×¤×•×© ××™×œ×” ×¢× ×¡×™×•××•×ª ×“×§×“×•×§×™×•×ª
  String _createSuffixRegexPattern(String word) {
    if (word.isEmpty) return word;
    // ×©×™××•×© ×‘×ª×‘× ×™×ª ×§×‘×•×¢×” ×•×™×¢×™×œ×” - ××•×’×‘×œ×ª ×œ×¡×™×•××•×ª × ×¤×•×¦×•×ª
    const suffixPattern =
        r'(×•×ª|×™×|×™×”|×™×•|×™×š|×™× ×•|×™×›×|×™×›×Ÿ|×™×”×|×™×”×Ÿ|×™|×š|×•|×”|× ×•|×›×|×›×Ÿ|×|×Ÿ)?';
    return RegExp.escape(word) + suffixPattern;
  }

  /// ×™×•×¦×¨ ×“×¤×•×¡ ×¨×’×§×¡ ×§×•××¤×§×˜×™ ×œ×—×™×¤×•×© ××™×œ×” ×¢× ×§×™×“×•××•×ª ×•×¡×™×•××•×ª ×“×§×“×•×§×™×•×ª ×™×—×“
  String _createFullMorphologicalRegexPattern(String word) {
    if (word.isEmpty) return word;
    // ×©×™×œ×•×‘ ×©×œ ×§×™×“×•××•×ª ×•×¡×™×•××•×ª - ××•×’×‘×œ ×œ× ×¤×•×¦×•×ª ×‘×™×•×ª×¨
    const prefixPattern = r'(×•|×|×›|×‘|×©|×œ|×”|×“)?(×›|×‘|×©|×œ|×”|×“)?(×”)?';
    const suffixPattern =
        r'(×•×ª|×™×|×™×”|×™×•|×™×š|×™× ×•|×™×›×|×™×›×Ÿ|×™×”×|×™×”×Ÿ|×™|×š|×•|×”|× ×•|×›×|×›×Ÿ|×|×Ÿ)?';
    return prefixPattern + RegExp.escape(word) + suffixPattern;
  }
}
